# 146. LRU Cache

## Tags

- Free
- Medium
- Frequency: :fire::fire::fire::fire::fire::fire::fire::snowflake::snowflake::snowflake:

## Links

[Leetcode](https://leetcode.com/problems/lru-cache/description/)

[Blog](http://206.81.6.248:12306/leetcode/lru-cache/description)

## Description

Design and implement a data structure for <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU" target="_blank">Least Recently Used (LRU) cache</a>. It should support the following operations: <code>get</code> and <code>put</code>.

<code>get(key)</code> - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.<br/>
<code>put(key, value)</code> - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.

The cache is initialized with a <strong>positive</strong> capacity.

<b>Follow up:</b><br/>Could you do both operations in <b>O(1)</b> time complexity?

<b>Example:</b>

LRUCache cache = new LRUCache( 2 /* capacity */ );  

cache.put(1, 1);  
cache.put(2, 2);  
cache.get(1);       // returns 1  
cache.put(3, 3);    // evicts key 2  
cache.get(2);       // returns -1 (not found)  
cache.put(4, 4);    // evicts key 1  
cache.get(1);       // returns -1 (not found)  
cache.get(3);       // returns 3  
cache.get(4);       // returns 4  

## Python code

```python
class ListNode:
    def __init__(self, val):
        self.val = val
        self.next = None
        self.prev = None

class DoubleLL(object):
    def __init__(self):
        # we need to consider the ”edge cases”.
        # for doubly linked lists, the edge cases are the first and last elements. These cases require special attention since head.prev and tail.next will be null which can cause errors in your methods if you are not careful.
        # to avoid such errors, it is common to define linked lists by using a “dummy” head node and a “dummy” tail node, instead of head and tail reference variables.
        self.dummy = ListNode(0)
        # we use tail here to return the least used vaule, which is the last element of the list
        self.tail = self.dummy
        # size does not count dummy
        self.size = 0

    def add_to_head(self, node):
        # connect the node and the head of the list
        node.next = self.dummy.next
        node.prev = self.dummy
        self.dummy.next = node
        # connect the node and the prior first node of the list, if any
        if node.next:
            node.next.prev = node
        # redirect tail
        if self.tail is self.dummy:
            self.tail = self.tail.next
        self.size += 1

    def del_node(self, node):
        if not node:
            return
        if node is self.tail:
            self.tail = self.tail.prev
        # remove the connection between the node and the former node
        node.prev.next = node.next
        # remove the connection between the node and the latter node
        if node.next:
            node.next.prev = node.prev
        self.size -= 1

class LRUCache(object):
    def __init__(self, size):
        self.queue = DoubleLL()
        self.dic = {}
        self.size = size

    def get(self, key: int) -> int:
        if key in self.dic:
            # update the status of the key
            node = self.dic[key]
            self.queue.del_node(node)
            self.queue.add_to_head(node)
            return node.val[1]
        return -1

    def put(self, key: int, value: int) -> None:
        # if reach the capacity, then remove the last element of the list
        if key not in self.dic and self.queue.size >= self.size:
            rm_node = self.queue.tail
            self.queue.del_node(rm_node)
            self.dic.pop(rm_node.val[0])
        # update the status of the key
        self.queue.del_node(self.dic.pop(key, None))
        self.queue.add_to_head(ListNode((key, value)))
        self.dic[key] = self.queue.dummy.next
```

## Visualization

pending gif...

## Reference

None
